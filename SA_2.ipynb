{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SA 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZHD3oBij3sl"
      },
      "source": [
        "# Sentiment analysis on movie reviews\n",
        "\n",
        "The scope of this project is to classify movie reviews into positive or negative class. First it is performed at document level then at sentence level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPe2nGsAkPOU"
      },
      "source": [
        "Download spacy english large corpus and load it without the unecessary pipeline processes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5P83dF8j-EO",
        "outputId": "d91cc5f0-7e3d-490f-f645-2c6ea85ee283"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=0865bd1ca5f332b3ec0cfefd1986894279bf983f1d9813eb8b018b7a2c09fb7a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sbve9g2w/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZuM_uIUCoeV"
      },
      "source": [
        "Import *en_core_web_lg*, this language model will be used to tokenize tokens and to split the reviews into single sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx2iZWBFkK6_"
      },
      "source": [
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load(disable=[\"tagger\", \"parser\", \"ner\", \"entity_linker\", \"entity_ruler\", \"textcat\", \"textcat_multilabel\", \"morphologizer\", \"attribute_ruler\", \"transformer\"])\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8_seBnAkgOO"
      },
      "source": [
        "Import used libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaqFMCrfkBRn",
        "outputId": "11cc6328-1ca1-4485-b9ab-42ccd36b62b9"
      },
      "source": [
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "from torchtext.datasets import IMDB"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_MYKkZRk2lG"
      },
      "source": [
        "Function used to preprocess one review. It makes text lowercase, remove html tags that could still be present due to the web scraping phase, removes punctuation, removes stop words and lemmatize the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKTM-XrakOWP"
      },
      "source": [
        "def data_preprocessing(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('<.*?>', '', text)  # Remove HTML from text\n",
        "  text = ''.join([c for c in text if c not in string.punctuation])  # Remove punctuation\n",
        "  text = [word for word in text.split() if word not in stop_words]\n",
        "  text = ' '.join(text)\n",
        "  return ' '.join([w.lemma_ for w in nlp(text)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWV8o0PblqF-"
      },
      "source": [
        "Function that vectorize the tokens of one review. The vecotorization used is implemented in the loaded spacy pipeline, it cosists of a vector of dimension 300 for each token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD6r9v6ClSiH"
      },
      "source": [
        "embedding_dim = 300  # the one used by spaCy\n",
        "\n",
        "\n",
        "def embed_one_review(review):\n",
        "  unknown = np.ones(embedding_dim, dtype=np.float32)\n",
        "  embedded_data = []\n",
        "  text = review[\"text\"]\n",
        "  tokens2vec = []\n",
        "  for word in text.split():\n",
        "    token2vec = nlp.vocab[word].vector.astype(np.float32)\n",
        "    if np.count_nonzero(token2vec) == 0:\n",
        "      token2vec = unknown\n",
        "    tokens2vec.append(token2vec.tolist())\n",
        "  review[\"text\"] = tokens2vec\n",
        "  return review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R6PFygcmIPt"
      },
      "source": [
        "Function that adds padding to the review if it is too short or truncate if it is too long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wUOGWkApsqE"
      },
      "source": [
        "chosen_length = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvUX6TvUmHkH"
      },
      "source": [
        "def padding_one_review(review, seq_len):\n",
        "  zero = list(np.zeros(embedding_dim, dtype=np.float32))\n",
        "  if len(review[\"text\"]) <= seq_len:\n",
        "    zeros = [zero for each in range(seq_len - len(review[\"text\"]))]\n",
        "    new = zeros + review[\"text\"]\n",
        "  else:\n",
        "    new = review[\"text\"][: seq_len]\n",
        "  new = {\n",
        "    \"text\": new,\n",
        "    \"sentiment\": review[\"sentiment\"]\n",
        "  }\n",
        "  return new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXpvQaKtnZe-"
      },
      "source": [
        "Load dataset and store it in two dictionaries, one for training and one for testing, composed like so:\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        "  \"text\": the text of the review,\n",
        "  \"sentiment\": 0 or 1\n",
        "}\n",
        "```\n",
        "0 is negative and 1 is positive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO_OezcOBuQ6"
      },
      "source": [
        "The dataset used is [IMDB Movie Reviews](https://pytorch.org/text/0.8.1/_modules/torchtext/datasets/imdb.html) provided by pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkiz9kHKnY8t",
        "outputId": "592eac0a-bc7e-4bd8-ba40-d20fc6a1f156"
      },
      "source": [
        "train_iter = IMDB(split='train')\n",
        "train_data = []\n",
        "for label, review in train_iter:\n",
        "  review = data_preprocessing(review)\n",
        "  review = {\n",
        "    \"text\": review,\n",
        "    \"sentiment\": 1 if label == \"pos\" else 0\n",
        "  }\n",
        "  train_data.append(review)\n",
        "print(len(train_data))\n",
        "print(train_data[0])\n",
        "\n",
        "test_iter = IMDB(split='test')\n",
        "val_data = []\n",
        "for label, review in test_iter:\n",
        "  review = data_preprocessing(review)\n",
        "  review = {\n",
        "    \"text\": review,\n",
        "    \"sentiment\": 1 if label == \"pos\" else 0\n",
        "  }\n",
        "  val_data.append(review)\n",
        "print(len(val_data))\n",
        "print(val_data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 78.6MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "{'text': 'rent curiousyellow video store controversy surround \\ufeff1 release 1967 also hear \\ufeff1 seize us custom ever try enter country therefore fan film consider controversial really see myselfthe plot center around young swedish drama student name lena want learn everything life particular want focus attention make sort documentary average swede think certain political issue vietnam war race issue unite state ask politician ordinary denizen stockholm opinion politic sex drama teacher classmate marry menwhat kill curiousyellow 40 year ago consider pornographic really sex nudity scene far even shoot like cheaply make porno countryman mind find shock reality sex nudity major staple swedish cinema even ingmar bergman arguably answer good old boy john ford sex scene filmsi commend filmmakers fact sex show film show artistic purpose rather shock people make money show pornographic theater america curiousyellow good film anyone want study meat potato pun intend swedish cinema really film do not much plot', 'sentiment': 0}\n",
            "25000\n",
            "{'text': 'love scifi will put lot scifi moviestv usually underfund underappreciated misunderstand try like really good tv scifi babylon 5 star trek original silly prosthetic cheap cardboard set stilt dialogue cg do not match background painfully onedimensional character can not overcome scifi set -PRON- be sure think babylon 5 good scifi tv clichéd uninspiring us viewer may like emotion character development scifi genre take seriously cf star trek may treat important issue yet serious philosophy really difficult care character simply foolish miss spark life action reaction wooden predictable often painful watch maker earth know rubbish always say gene roddenberrys earth otherwise people would continue watch roddenberrys ash must turn orbit dull cheap poorly edit watch without advert break really bring home trudge trabant show lumber space spoiler kill main character bring back another actor jeeez dallas', 'sentiment': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkApCNKaqCrN"
      },
      "source": [
        "Create dataset by inheriting from Dataset class from the pytorch library. The embedding and padding operations are performed during the get item because having in RAM all the embedded reviews isn't possible in google collab due to excessive memory usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp3LkaKXqC4N"
      },
      "source": [
        "class SentimentDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews):\n",
        "    self.reviews = reviews\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, idx):  # TODO if the bottleneck is here and we cannot do this operation before due to memory issues, try to do this operation only for the first epoch\n",
        "    review = copy.deepcopy(self.reviews[idx])\n",
        "    review = embed_one_review(review)\n",
        "    review = padding_one_review(review, chosen_length)\n",
        "    return torch.tensor(review[\"text\"]), torch.tensor(review[\"sentiment\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64Zci1fZq-p8"
      },
      "source": [
        "# create tensor dataset\n",
        "train_dataset = SentimentDataset(copy.deepcopy(train_data))\n",
        "valid_dataset = SentimentDataset(copy.deepcopy(val_data))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=False)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIK5n2ZJrF-E"
      },
      "source": [
        "Create the model that will process the documents. It is a GRU stacked with two fully connected layers with a sigmoid as output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avha-TYOrEzo"
      },
      "source": [
        "class sentimentGRU(nn.Module):\n",
        "  \"\"\"\n",
        "  The RNN model that will be used to perform Sentiment analysis.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, embedding_dim, hidden_dim, n_layers, drop_prob=0.5, output_size=1):\n",
        "    \"\"\"\n",
        "    Initialize the model by setting up the layers.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # GRU layer\n",
        "    self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True, bidirectional=True)\n",
        "\n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    \n",
        "    # Non linearity\n",
        "    self.leaky_ReLU = nn.LeakyReLU(0.1)\n",
        "\n",
        "    # Linear and sigmoid layers\n",
        "    self.fc_in = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.fc_out = nn.Linear(hidden_dim, output_size)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    \"\"\"\n",
        "    Perform a forward pass of our model on some input and hidden state.\n",
        "    \"\"\"\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    # embedding and gru_out\n",
        "    gru_out, hidden = self.gru(x, hidden)\n",
        "\n",
        "    # stack up gru outputs\n",
        "    gru_out = gru_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "    # Dropout and fully connected layer\n",
        "    out = self.dropout(gru_out)\n",
        "    out = self.fc_in(out)\n",
        "    out = self.leaky_ReLU(out)\n",
        "    out = self.dropout(out)\n",
        "    out = self.fc_out(out)\n",
        "\n",
        "    # sigmoid function\n",
        "    sig_out = self.sigmoid(out)\n",
        "\n",
        "    # reshape to be batch size first\n",
        "    sig_out = sig_out.view(batch_size, -1)\n",
        "    sig_out = sig_out[:, -1] # get last batch of labels\n",
        "\n",
        "    return sig_out, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    \"\"\"\n",
        "    Initializes hidden state\n",
        "    \"\"\"\n",
        "    # Create a tensor with sizes n_layers x batch_size x hidden_dim,\n",
        "    # initialized to zero, for hidden state\n",
        "    # number of layers is multiplied by 2 because it is bidirectional GRU\n",
        "    h0 = torch.zeros((self.n_layers*2, batch_size,self.hidden_dim)).to(device)\n",
        "    hidden = h0\n",
        "    return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUX-1bsJr9c-"
      },
      "source": [
        "Instantiate the model with 3 layers in the GRU and 256 as hidden dimensions, these values have been chosen heuristically"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYu_b1ZEMWBE",
        "outputId": "458e66fa-2dc3-4201-90ad-83b13f23ed22"
      },
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"GPU is available\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"GPU not available, CPU used\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66U2VV5ur8X1",
        "outputId": "368a880d-b337-4f25-86ea-e6228dddf0be"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "output_size = 1\n",
        "hidden_dim = 256\n",
        "n_layers = 3\n",
        "\n",
        "model = sentimentGRU(embedding_dim, hidden_dim, n_layers, output_size=output_size)\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentimentGRU(\n",
            "  (gru): GRU(300, 256, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (leaky_ReLU): LeakyReLU(negative_slope=0.1)\n",
            "  (fc_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (fc_out): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bLwljSpsrdm"
      },
      "source": [
        "Define training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_s3GaLcs2pW"
      },
      "source": [
        "def train(model, train_loader, batch_size, criterion, optimizer):\n",
        "  train_losses = []\n",
        "  train_acc = 0.0\n",
        "  model.train()\n",
        "  # initialize hidden state \n",
        "  h = model.init_hidden(batch_size).to(device)\n",
        "\n",
        "  # store data for f1 score\n",
        "  true_positive = 0\n",
        "  false_negative = 0\n",
        "  all_positive = 0\n",
        "  for inputs, labels in train_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)   \n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    if inputs.size(0) < batch_size:\n",
        "      h = model.init_hidden(inputs.size(0)).to(device)\n",
        "    # h = tuple([each.data for each in h])\n",
        "    h = h.detach().clone().to(device)\n",
        "    output, h = model(inputs, h)\n",
        "\n",
        "    # calculate the loss and perform backprop\n",
        "    loss = criterion(output.squeeze(), labels.float())\n",
        "    loss.backward()\n",
        "    train_losses.append(loss.item())\n",
        "    # calculating accuracy\n",
        "    accuracy = acc(output,labels)\n",
        "    train_acc += accuracy\n",
        "    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    for output_idx, pred in enumerate(output):\n",
        "      pred = int(torch.round(pred).item())\n",
        "      label = labels[output_idx].item()\n",
        "      if label == 1 and pred == label:\n",
        "        true_positive += 1\n",
        "      if pred == 0 and label == 1:\n",
        "        false_negative += 1\n",
        "      if pred == 1:\n",
        "        all_positive += 1\n",
        "\n",
        "  precision = true_positive / all_positive if all_positive != 0 else 0\n",
        "  recall = true_positive / (true_positive + false_negative)\n",
        "  f1_score_train = 2 * (precision * recall) / (precision + recall)\n",
        "  return train_losses, train_acc, f1_score_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drznPWjzstM9"
      },
      "source": [
        "Define evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFaws8lMJvtT"
      },
      "source": [
        "def eval(model, valid_loader, batch_size, criterion):\n",
        "  true_positive = 0\n",
        "  false_negative = 0\n",
        "  all_positive = 0\n",
        "  val_h = model.init_hidden(batch_size).to(device)\n",
        "  val_losses = []\n",
        "  val_acc = 0.0\n",
        "  model.eval()\n",
        "  for inputs, labels in valid_loader:\n",
        "    # val_h = tuple([each.data for each in val_h])\n",
        "    val_h = val_h.detach().clone().to(device)\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    if inputs.size(0) < batch_size:\n",
        "      val_h = model.init_hidden(inputs.size(0)).to(device)\n",
        "\n",
        "    output, val_h = model(inputs, val_h)\n",
        "    val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "    val_losses.append(val_loss.item())\n",
        "        \n",
        "    accuracy = acc(output,labels)\n",
        "    val_acc += accuracy\n",
        "        \n",
        "    for output_idx, pred in enumerate(output):\n",
        "      pred = int(torch.round(pred).item())\n",
        "      label = labels[output_idx].item()\n",
        "      if label == 1 and pred == label:\n",
        "        true_positive += 1\n",
        "      if pred == 0 and label == 1:\n",
        "        false_negative += 1\n",
        "      if pred == 1:\n",
        "        all_positive += 1\n",
        "  precision = true_positive / all_positive if all_positive != 0 else 0\n",
        "  recall = true_positive / (true_positive + false_negative)\n",
        "  f1_score_val = 2 * (precision * recall) / (precision + recall)\n",
        "  return val_losses, val_acc, f1_score_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E6EoXDxsnsU"
      },
      "source": [
        "## Start the training and evaluation loop\n",
        "\n",
        "The following code initializes the optimizer, loss function and all the variables needed to compute accuracy and f1 score.\n",
        "\n",
        "The default number of epochs is 5 because I empirically seen that after the fourth epoch the network start to overfit.\n",
        "\n",
        "A mecanism of early stopping is implemented by storing the weights of the network in a variable when the validation f1 score is greater than the current maximum. So only the weights that perform better on the validation dataset are kept."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "id": "nYxXDiFusl3d",
        "outputId": "5a3ea94d-50dc-4f41-94a4-84e01fa7d92f"
      },
      "source": [
        "saved_model_state_dict = None\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "\n",
        "# function to compute accuracy\n",
        "def acc(pred,label):\n",
        "  pred = torch.round(pred.squeeze())\n",
        "  return torch.sum(pred == label.squeeze()).item()\n",
        "\n",
        "clip = 5\n",
        "epochs = 5\n",
        "valid_loss_min = np.Inf\n",
        "f1_score_max = 0\n",
        "\n",
        "# train for some number of epochs\n",
        "epoch_tr_loss,epoch_vl_loss = [],[]\n",
        "epoch_tr_acc,epoch_vl_acc = [],[]\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_losses, train_acc, f1_score_train = train(model, train_loader, batch_size, criterion, optimizer)\n",
        "\n",
        "  val_losses, val_acc, f1_score_val = eval(model, valid_loader, batch_size, criterion)\n",
        "\n",
        "  epoch_train_loss = np.mean(train_losses)\n",
        "  epoch_val_loss = np.mean(val_losses)\n",
        "  epoch_train_acc = train_acc/len(train_loader.dataset)\n",
        "  epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
        "  epoch_tr_loss.append(epoch_train_loss)\n",
        "  epoch_vl_loss.append(epoch_val_loss)\n",
        "  epoch_tr_acc.append(epoch_train_acc)\n",
        "  epoch_vl_acc.append(epoch_val_acc)\n",
        "  print(f'Epoch {epoch+1}') \n",
        "  print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
        "  print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
        "\n",
        "  \n",
        "  print(f\"f1 score training: {f1_score_train} f1 score validation: {f1_score_val}\")\n",
        "\n",
        "  if f1_score_val >= f1_score_max:\n",
        "    print('f1 score increased ({:.6f} --> {:.6f}).  Saving model ...'.format(f1_score_max,f1_score_val))\n",
        "    saved_model_state_dict = model.state_dict()\n",
        "    f1_score_max = f1_score_val\n",
        "  print(40*'==')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "train_loss : 0.4925702843739062 val_loss : 0.333573760122669\n",
            "train_accuracy : 76.34 val_accuracy : 85.66\n",
            "f1 score training: 0.7666574618328139 f1 score validation: 0.8574949318281194\n",
            "f1 score increased (0.000000 --> 0.857495).  Saving model ...\n",
            "================================================================================\n",
            "Epoch 2\n",
            "train_loss : 0.32122921251824926 val_loss : 0.3176987685111104\n",
            "train_accuracy : 86.44800000000001 val_accuracy : 86.70400000000001\n",
            "f1 score training: 0.864804469273743 f1 score validation: 0.857375783060156\n",
            "================================================================================\n",
            "Epoch 3\n",
            "train_loss : 0.28345000188873737 val_loss : 0.2903445279415773\n",
            "train_accuracy : 88.444 val_accuracy : 87.652\n",
            "f1 score training: 0.8847488730203056 f1 score validation: 0.8700593509281475\n",
            "f1 score increased (0.857495 --> 0.870059).  Saving model ...\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0625e6b5d07a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-71656bc25115>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, batch_size, criterion, optimizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mfalse_negative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mall_positive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Creating new variables for the hidden state, otherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbefqSvd3_vM"
      },
      "source": [
        "# Sentence level document classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgMj7ljd2dUw"
      },
      "source": [
        "Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gGmGWKL4zH8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca16253-3804-4d35-8075-ae9478837b7f"
      },
      "source": [
        "import numpy\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjWgitNR2hEf"
      },
      "source": [
        "Define a function that preprocess the reviews, it removes every HTML tags left from the web scraping phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8jSFNTY4xPS"
      },
      "source": [
        "def sentence_preprocessing(text):\n",
        "  text = re.sub('<.*?>', '', text)  # Remove HTML from text\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5RNP9E84v-6"
      },
      "source": [
        "vaderAnalyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "sentiments = []\n",
        "reviews = []\n",
        "for label, review in IMDB(split='test'):\n",
        "  if label == \"neg\":\n",
        "    sentiments.append(0)\n",
        "  else:\n",
        "    sentiments.append(1)\n",
        "  reviews.append(sentence_preprocessing(review))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RZA-Of_kKrl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dbd6c18-7b39-47ab-9951-a8d40e4f716b"
      },
      "source": [
        "list_of_sentences = []\n",
        "for review_idx, review in enumerate(reviews):\n",
        "  list_of_sentences.append([])\n",
        "  for sent in nlp(review).sents:\n",
        "    list_of_sentences[review_idx].append(sent.text)\n",
        "print(list_of_sentences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I love sci-fi and am willing to put up with a lot.', 'Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood.', 'I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original).', \"Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (\", \"I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV.\", \"It's not.\", \"It's clichéd and uninspiring.)\", 'While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf.', 'Star Trek).', 'It may treat important issues, yet not as a serious philosophy.', \"It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life.\", 'Their actions and reactions are wooden and predictable, often painful to watch.', 'The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching.', \"Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space.\", 'Spoiler.', 'So, kill off a main character.', 'And then bring him back as another actor.', 'Jeeez!', 'Dallas all over again.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqpjLV4Q1kRL"
      },
      "source": [
        "**Classification**\n",
        "\n",
        "Two approaches has been tried, befre the classification phase the list of list of scores is stored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGfpUEn004LK"
      },
      "source": [
        "This first method classify each sentence into positive or negative by looking at the score given by VADER. Then the entire review is classified as positive if the number of negative sentences is less then the number of positive ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqZirqtm4pyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a997d31-f5e6-4ea6-bd31-9d3be2b871b7"
      },
      "source": [
        "scores_predicted = []\n",
        "for review_idx, doc in enumerate(list_of_sentences):\n",
        "  scores_predicted.append([])\n",
        "  for sent in doc:\n",
        "    scores_predicted[review_idx].append(vaderAnalyzer.polarity_scores(sent))\n",
        "\n",
        "\n",
        "sentiments_predicted_text = []\n",
        "for review_idx, doc in enumerate(scores_predicted):\n",
        "  sentiments_predicted_text.append([])\n",
        "  for sent_score in doc:\n",
        "    if sent_score['neg'] > sent_score['pos']:\n",
        "      sentiments_predicted_text[review_idx].append('neg')\n",
        "    else:\n",
        "      sentiments_predicted_text[review_idx].append('pos')\n",
        "\n",
        "\n",
        "sentiments_predicted = []\n",
        "for doc in sentiments_predicted_text:\n",
        "  if doc.count('neg') > doc.count('pos'):\n",
        "    sentiments_predicted.append(0)\n",
        "  else:\n",
        "    sentiments_predicted.append(1)\n",
        "\n",
        "print(classification_report(sentiments, sentiments_predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.25      0.38     12500\n",
            "           1       0.56      0.95      0.70     12500\n",
            "\n",
            "    accuracy                           0.60     25000\n",
            "   macro avg       0.70      0.60      0.54     25000\n",
            "weighted avg       0.70      0.60      0.54     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIuA99PG17YX"
      },
      "source": [
        "This second approach takes into account of the actual value of the score given by VADER and sum all of them to get a final score for all positive contribution and negative contribution. If the summed positive scores is greater than the summed negative the review is considered as positive.\n",
        "\n",
        "This approach allows me to not account for objective sentences, because objective sentences have very low positve and negative values, actually giving poor contribution to the classification, hence a great bust in performance with respect the previous attempt with V.A.D.E.R."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWCvnmZR172f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f80e08b-60ea-4c38-db96-3d9d867c055e"
      },
      "source": [
        "sentiments_predicted = []\n",
        "for doc in scores_predicted:\n",
        "  pos = 0.0\n",
        "  neg = 0.0\n",
        "  for sent_score in doc:\n",
        "    pos += sent_score['pos']\n",
        "    neg += sent_score['neg']\n",
        "  sentiments_predicted.append(0 if neg > pos else 1)\n",
        "\n",
        "print(classification_report(sentiments, sentiments_predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.55      0.66     12500\n",
            "           1       0.66      0.88      0.75     12500\n",
            "\n",
            "    accuracy                           0.71     25000\n",
            "   macro avg       0.74      0.71      0.71     25000\n",
            "weighted avg       0.74      0.71      0.71     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}